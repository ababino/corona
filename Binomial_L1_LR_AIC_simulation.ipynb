{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.integrate\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from statsmodels.sandbox.regression.predstd import wls_prediction_std\n",
    "from sklearn import linear_model\n",
    "from scipy import stats as sps\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "locator = mdates.AutoDateLocator()\n",
    "formatter = mdates.ConciseDateFormatter(locator)\n",
    "GAMMA = 1/7.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dS(beta, S, I, N):\n",
    "    return -beta * S * I / N\n",
    "def dI(beta, gamma, S, I, N):\n",
    "    return beta * S * I / N - gamma * I\n",
    "def dR(gamma, S, I, N):\n",
    "    return gamma * I\n",
    "\n",
    "def dY(x, t):\n",
    "    N = 10000\n",
    "    S = x[0]\n",
    "    I = x[1]\n",
    "    R = x[2]\n",
    "    beta = 2 * GAMMA\n",
    "    gamma = GAMMA\n",
    "    if t>20 and beta == 2 * GAMMA:\n",
    "        beta = 1.5 * GAMMA\n",
    "    if t>40 and beta == 1.5 * GAMMA:\n",
    "        beta = 0.7 * GAMMA\n",
    "    if t>60 and beta == 0.7 * GAMMA:\n",
    "        beta = 0.4 * GAMMA\n",
    "    #if t>80 and beta == 0.7 * GAMMA:\n",
    "    #    beta *= 0. * GAMMA\n",
    "\n",
    "    dx0 = dS(beta, S, I, N)\n",
    "    dx1 = dI(beta, gamma, S, I, N)\n",
    "    dx2 = dR(gamma, S, I, N)\n",
    "    return np.array([dx0, dx1, dx2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf6ce1f3dde441a4a1483c720df9bce5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c17456fcaa4240e38fe1ab129c906830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.052824121487497926\n",
      "0.23076923076923084\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1234)\n",
    "plt.close('all')\n",
    "dsname = 'SIR'\n",
    "t = np.arange(0, 100, 1)\n",
    "r0 = 1\n",
    "y = scipy.integrate.odeint(dY, np.array([10000-r0, r0, 0]), t)\n",
    "df = pd.DataFrame(y, columns=['S', 'Positive', 'R'], index=t)\n",
    "df['Negative'] = 1000#df['S']+df['R']\n",
    "df['Odds'] = df['Positive']/df['Negative']\n",
    "#df['Odds'] += np.random.randn(len(df))*df['Odds']*0.05\n",
    "df['logodds'] =  np.log(df['Odds']) +  np.random.randn(len(df))*0.05\n",
    "df['Odds'] = np.exp(df['logodds'])\n",
    "df['Date'] = t\n",
    "plt.scatter(df.index, df['Odds'])\n",
    "plt.yscale('log')\n",
    "plt.ylim([0.0005, 0.1])\n",
    "plt.grid(True)\n",
    "#plt.figure()\n",
    "df.plot(y=['S', 'Positive', 'R'])\n",
    "print((df['Odds']/(1+df['Odds'])).max())\n",
    "print(1-1/1.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FirstInChunkSelector(object):\n",
    "    '''Selects first element from each non zero chunk.'''\n",
    "\n",
    "    def __init__(self, clf):\n",
    "        self.clf = clf\n",
    "        self.coef = None\n",
    "        self.mask = None\n",
    "\n",
    "    def select_coef(self):\n",
    "        n_features = len(self.clf.coef_)\n",
    "        no_zero = np.zeros(n_features+1)\n",
    "        no_zero[1:] = self.clf.coef_ != 0\n",
    "        self.mask = np.diff(no_zero)>0\n",
    "        self.coef = self.clf.coef_[self.mask]\n",
    "        return self.coef\n",
    "\n",
    "    def transform(self, X):\n",
    "        self.select_coef()\n",
    "        return X[:, self.mask]\n",
    "\n",
    "    def get_support(self):\n",
    "        self.select_coef()\n",
    "        return self.mask\n",
    "\n",
    "    def get_number_of_features(self):\n",
    "        self.select_coef()\n",
    "        return sum(self.mask)\n",
    "\n",
    "\n",
    "class LassoICSelector(object):\n",
    "    \"\"\"LASSO regression with FirstInChunk selector.\"\"\"\n",
    "\n",
    "    def __init__(self, X, y, criterion):\n",
    "        self.lasso = linear_model.LassoLars(alpha=0, max_iter=100000)\n",
    "        self.criterion = criterion\n",
    "        self.selector = FirstInChunkSelector(self.lasso)\n",
    "        self.OLS = sm.OLS\n",
    "        self.ols = self.OLS(y, X)\n",
    "        self.ols_results = None\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.final_ols = False\n",
    "\n",
    "    def transform_to_ols(self, X):\n",
    "        '''Selects only the features of X are used by OLS.\n",
    "        Also, adds a coloumn with ones for the intercept.\n",
    "        '''\n",
    "\n",
    "        X_new = self.selector.transform(X)\n",
    "        if self.final_ols:\n",
    "            X_new = X[:, self.support]\n",
    "        X_new_with_cte = np.hstack([X_new, np.ones((X_new.shape[0], 1))])\n",
    "        return X_new_with_cte\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        '''Selects features and fits the OLS.'''\n",
    "\n",
    "        # select features\n",
    "        X_new = self.transform_to_ols(X)\n",
    "\n",
    "        # fit ols\n",
    "        self.ols = self.OLS(y, X_new)\n",
    "        self.ols_results = self.ols.fit()\n",
    "\n",
    "        # remove non signicative variables and fit again\n",
    "        mask = self.ols_results.pvalues < 0.05 / len(self.ols_results.pvalues)\n",
    "        Xnew = self.transform_to_ols(X)\n",
    "        Xnew = Xnew[:, mask]\n",
    "        self.support = self.selector.get_support()\n",
    "        self.ols = self.OLS(y, Xnew)\n",
    "        self.ols_results = self.ols.fit()\n",
    "        while any(self.ols_results.pvalues >= 0.05 / len(self.ols_results.pvalues)):\n",
    "            mask.values[mask.values] = (self.ols_results.pvalues < 0.05 / len(self.ols_results.pvalues)).values\n",
    "            Xnew = self.transform_to_ols(X)\n",
    "            Xnew = Xnew[:, mask]\n",
    "            self.support = self.selector.get_support()\n",
    "            self.ols = self.OLS(y, Xnew)\n",
    "            self.ols_results = self.ols.fit()\n",
    "        self.support[self.support] = mask[:-1]\n",
    "\n",
    "    def fit_best_alpha(self, X, y):\n",
    "        '''returns the model with the lowst cirterion.'''\n",
    "\n",
    "        self.lasso.fit(X, y)\n",
    "        alphas = self.lasso.alphas_\n",
    "        self.criterions_ = np.zeros(len(alphas))\n",
    "        self.log_liklehods = np.zeros(len(alphas))\n",
    "        \n",
    "        \n",
    "        for i, alpha in enumerate(alphas):\n",
    "            self.lasso.coef_ = self.lasso.coef_path_[:, i]\n",
    "            self.fit(X, y)\n",
    "            self.criterions_[i], self.log_liklehods[i] = self.get_criterion(self.ols.exog, y)\n",
    "        \n",
    "        # we use a list of tuples to find the minimum cirterion value.\n",
    "        # If there are ties, we use the maximum alpha value.\n",
    "        criterions_idx = list(zip(self.criterions_, alphas, range(len(alphas))))\n",
    "        criterion, alpha, idx = min(criterions_idx, key=lambda x: (x[0], -x[1]))\n",
    "        \n",
    "        self.lasso.coef_ = self.lasso.coef_path_[:, idx]\n",
    "        self.lasso.alpha = alpha\n",
    "        self.fit(X, y)\n",
    "        self.final_ols = True\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''Predicts y useing the OLS fit.'''\n",
    "\n",
    "        return self.ols.predict(self.ols_results.params, X)\n",
    "\n",
    "    def log_liklihood(self, X, y):\n",
    "        '''Computes the log liklihood assuming normally distributed errors.'''\n",
    "\n",
    "        eps64 = np.finfo('float64').eps\n",
    "\n",
    "        # residuals\n",
    "        R = y - self.predict(X)\n",
    "        sigma2 = np.var(R)\n",
    "\n",
    "        loglike = -0.5 * len(R) * np.log(sigma2)\n",
    "        loglike -= 0.5 * len(R) * np.log(2*np.pi) - 0.5*len(R) + 0.5\n",
    "        return loglike\n",
    "\n",
    "    def get_criterion(self, X, y):\n",
    "        '''Computes AIC or BIC criterion.'''\n",
    "\n",
    "        n_samples = X.shape[0]\n",
    "        if self.criterion == 'aic':\n",
    "            K = 2  # AIC\n",
    "        elif self.criterion == 'bic':\n",
    "            K = np.log(n_samples)\n",
    "        else:\n",
    "            raise ValueError('criterion should be either bic or aic')\n",
    "\n",
    "        log_like = self.log_liklihood(X, y)\n",
    "        df = X.shape[1]\n",
    "\n",
    "        aic = K * df - 2*log_like\n",
    "        self.criterion_ = aic\n",
    "\n",
    "        return self.criterion_, log_like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c79cffdadc054965932eace7726e9719",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>Odds</td>       <th>  R-squared:         </th> <td>   0.998</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.998</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>1.476e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 18 Jun 2020</td> <th>  Prob (F-statistic):</th> <td>9.10e-132</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>16:14:52</td>     <th>  Log-Likelihood:    </th> <td>  160.57</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   100</td>      <th>  AIC:               </th> <td>  -311.1</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    95</td>      <th>  BIC:               </th> <td>  -298.1</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     4</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    0.1323</td> <td>    0.001</td> <td>   91.043</td> <td> 0.000</td> <td>    0.129</td> <td>    0.135</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>   -0.0637</td> <td>    0.002</td> <td>  -26.134</td> <td> 0.000</td> <td>   -0.069</td> <td>   -0.059</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>   -0.1061</td> <td>    0.002</td> <td>  -50.245</td> <td> 0.000</td> <td>   -0.110</td> <td>   -0.102</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>   -0.0436</td> <td>    0.001</td> <td>  -29.332</td> <td> 0.000</td> <td>   -0.047</td> <td>   -0.041</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>   -6.9009</td> <td>    0.020</td> <td> -346.724</td> <td> 0.000</td> <td>   -6.940</td> <td>   -6.861</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>10.072</td> <th>  Durbin-Watson:     </th> <td>   2.456</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.007</td> <th>  Jarque-Bera (JB):  </th> <td>  10.317</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.656</td> <th>  Prob(JB):          </th> <td> 0.00575</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.869</td> <th>  Cond. No.          </th> <td>    305.</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                   Odds   R-squared:                       0.998\n",
       "Model:                            OLS   Adj. R-squared:                  0.998\n",
       "Method:                 Least Squares   F-statistic:                 1.476e+04\n",
       "Date:                Thu, 18 Jun 2020   Prob (F-statistic):          9.10e-132\n",
       "Time:                        16:14:52   Log-Likelihood:                 160.57\n",
       "No. Observations:                 100   AIC:                            -311.1\n",
       "Df Residuals:                      95   BIC:                            -298.1\n",
       "Df Model:                           4                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "x1             0.1323      0.001     91.043      0.000       0.129       0.135\n",
       "x2            -0.0637      0.002    -26.134      0.000      -0.069      -0.059\n",
       "x3            -0.1061      0.002    -50.245      0.000      -0.110      -0.102\n",
       "x4            -0.0436      0.001    -29.332      0.000      -0.047      -0.041\n",
       "const         -6.9009      0.020   -346.724      0.000      -6.940      -6.861\n",
       "==============================================================================\n",
       "Omnibus:                       10.072   Durbin-Watson:                   2.456\n",
       "Prob(Omnibus):                  0.007   Jarque-Bera (JB):               10.317\n",
       "Skew:                          -0.656   Prob(JB):                      0.00575\n",
       "Kurtosis:                       3.869   Cond. No.                         305.\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the independent and the dependent variables\n",
    "y = np.log(df['Odds'])\n",
    "X = np.tri(len(y))\n",
    "X = np.cumsum(X, axis=0)[:, 1:]\n",
    "X = X[df.Odds.notna(), :]\n",
    "y = y[df.Odds.notna()]\n",
    "\n",
    "# create lasso instance\n",
    "lics = LassoICSelector(X, y.values, 'bic')\n",
    "\n",
    "# fit\n",
    "lics.fit_best_alpha(X, y)\n",
    "\n",
    "plt.close('all')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(lics.lasso.alphas_, lics.criterions_)\n",
    "plt.scatter(lics.lasso.alphas_, lics.criterions_)\n",
    "plt.vlines(lics.lasso.alpha, min(lics.criterions_), max(lics.criterions_))\n",
    "plt.title('alpha={}'.format(lics.lasso.alpha))\n",
    "plt.ylabel('AIC')\n",
    "plt.xlabel('Alpha')\n",
    "plt.xscale('log')\n",
    "\n",
    "lics.ols_results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R</th>\n",
       "      <th>R_l</th>\n",
       "      <th>R_u</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.992245</td>\n",
       "      <td>1.970448</td>\n",
       "      <td>2.014042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.514720</td>\n",
       "      <td>1.495907</td>\n",
       "      <td>1.533533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.719285</td>\n",
       "      <td>0.703235</td>\n",
       "      <td>0.735334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.392354</td>\n",
       "      <td>0.383783</td>\n",
       "      <td>0.400925</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           R       R_l       R_u\n",
       "0   1.992245  1.970448  2.014042\n",
       "20  1.514720  1.495907  1.533533\n",
       "39  0.719285  0.703235  0.735334\n",
       "59  0.392354  0.383783  0.400925"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = df.copy()\n",
    "\n",
    "#yhat = lics.ols_results.fittedvalues\n",
    "y = np.log(df['Odds'])\n",
    "X = np.tri(len(y))\n",
    "X = np.cumsum(X, axis=0)[:, 1:]\n",
    "Xols = lics.transform_to_ols(X)\n",
    "yhat = lics.ols.predict(lics.ols_results.params, Xols)\n",
    "# from equation 5\n",
    "odds_hat = np.exp(yhat)\n",
    "\n",
    "# the error in yhat is\n",
    "#Xols = lics.transform_to_ols(X)\n",
    "(yhat_std, yhat_l, yhat_u) = wls_prediction_std(lics.ols_results, Xols)\n",
    "\n",
    "# propagation of errors\n",
    "oddshat_std = odds_hat*yhat_std\n",
    "\n",
    "data.loc[:, 'odds_hat'] = odds_hat\n",
    "data.loc[:, 'oddshat_std'] = oddshat_std\n",
    "data.loc[:, 'oddshat_l'] = odds_hat - 2*oddshat_std\n",
    "data.loc[:, 'oddshat_u'] = odds_hat + 2*oddshat_std\n",
    "\n",
    "# use coefficients to calculate Rt\n",
    "coef = np.zeros(len(data))\n",
    "coef_std = np.zeros_like(coef) * np.nan\n",
    "ind = np.squeeze(np.argwhere(lics.support))\n",
    "\n",
    "# we do not use the last coefficient since it's the intercept (=log(odds_0))\n",
    "coef[ind] = lics.ols_results.params[:-1]\n",
    "\n",
    "# using equation 2, 4 and 6\n",
    "data.loc[:, 'R'] = np.cumsum(coef)/GAMMA+1\n",
    "\n",
    "# get covarinace matrix of coefficients\n",
    "cov = lics.ols_results.cov_params().values\n",
    "\n",
    "# since the values of Rts are a sum of variables, we use the formula \n",
    "# of the sum of gaussian variables with a known covariance matrix \n",
    "stds = [np.sqrt(cov[:n, :n].sum()) for n in range(1, cov.shape[0])]\n",
    "\n",
    "coef_std[ind] = stds\n",
    "\n",
    "# error propagation formula\n",
    "data.loc[:, 'Rstd'] = coef_std / GAMMA\n",
    "\n",
    "data['Rstd'] = data['Rstd'].fillna(method='ffill')\n",
    "data['R_l'] = data['R'] - 2*data['Rstd']\n",
    "data['R_u'] = data['R'] + 2*data['Rstd']\n",
    "\n",
    "r_index = data.R.diff() != 0\n",
    "Rts = data.loc[r_index, ['R', 'R_l', 'R_u']]\n",
    "display(Rts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d347feb893264f15ac3867a3efdcdbf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.close('all')\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=[8, 4])\n",
    "\n",
    "ax = data.plot(x='Date', y='R', legend=False, ax=axes[0])\n",
    "ax.fill_between(data.index, data['R_u'], data['R_l'],\n",
    "                facecolor='blue', alpha=0.2, label='95% CI')\n",
    "\n",
    "\n",
    "ax.set_ylabel('Rt')\n",
    "ax.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "#plt.savefig('../figs/{}_RtL1.jpg'.format(dsname), dpi=300)\n",
    "#plt.show()\n",
    "\n",
    "ax = sns.scatterplot(x='Date', y='Odds', data=data, label='Data', ax=axes[1])\n",
    "ax = sns.lineplot(x='Date', y='odds_hat', label='Fit', ax=ax, data=data)\n",
    "ax.fill_between(data.index, data['oddshat_l'],\n",
    "                data['oddshat_u'],\n",
    "                facecolor='blue', alpha=0.1, label='95% CI')\n",
    "\n",
    "ax.legend()\n",
    "ax.set_yscale('log')\n",
    "\n",
    "#ax.xaxis.set_major_locator(locator)\n",
    "#ax.xaxis.set_major_formatter(formatter)\n",
    "\n",
    "plt.ylabel('Odds')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "ax.set_xlim(data['Date'].min(), data['Date'].max())\n",
    "plt.savefig('../figs/{}_OddsL1.jpg'.format(dsname), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c0743ce2a634525a9432d299819a179",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.close('all')\n",
    "fig, axes = plt.subplots(ncols=2, figsize=[10, 5])\n",
    "ax = data.plot(x='Date', y='R', legend=False, ax=axes[1])\n",
    "axes[1].fill_between(data.index, data['R_u'], data['R_l'],\n",
    "                facecolor='blue', alpha=0.2, label='95% CI')\n",
    "if dsname=='New York':\n",
    "    axes[1].vlines(events, 0, data.R_u.max(), linestyle='--')\n",
    "\n",
    "axes[1].set_ylabel('$R_t$')\n",
    "plt.grid(True)\n",
    "\n",
    "ax = sns.scatterplot(x='Date', y='Odds', data=data, label='Data', ax=axes[0])\n",
    "ax = sns.lineplot(x='Date', y='odds_hat', label='Fit', ax=axes[0], data=data)\n",
    "ax.fill_between(data.index, data['oddshat_l'],\n",
    "                data['oddshat_u'],\n",
    "                facecolor='blue', alpha=0.1, label='95% CI')\n",
    "\n",
    "ax.legend()\n",
    "axes[0].set_ylabel('Odds')\n",
    "\n",
    "axes[0].set_yscale('log')\n",
    "ax.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figs/{}_OddsL1.jpg'.format(dsname), dpi=300)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
